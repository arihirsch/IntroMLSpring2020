{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MidTerm Assignment: notebook 2: Backpropagation (Total 25pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><font color='red'><b>Given date : March 30</b></font></p>\n",
    "\n",
    "<font color='red'><b>Due date : April 17</b></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"backpropImage1.jpg\",width=400> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Training the network through backpropagation (Total 22pts)\n",
    "\n",
    "In this exercise, you will get to apply what you learned on backpropagation. \n",
    "We are interested in learning a classifier for the dataset below. This dataset is similar to what we did in class in the on hidden layer case. The difference is that you now have to learn a neural network with __multiple hidden layers__. We want to train the network through a minimization of the binary cross entropy. We further want to consider an $\\ell_2$-regularizarion term on all the weights except the bias, of each neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbkIICQaFmxBCgvF/yyqXhjwTU5gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as sio\n",
    "data_class1 = sio.loadmat('Notebook1_Ex1_Class1.mat')\n",
    "data_class2 = sio.loadmat('Notebook1_Ex1_Class2.mat')\n",
    "\n",
    "data_class1 = data_class1['Notebook1_Ex1_Class1']\n",
    "data_class2 = data_class2['Notebook1_Ex1_Class2']\n",
    "\n",
    "plt.scatter(data_class1[:,0], data_class1[:,1], c = 'r')\n",
    "plt.scatter(data_class2[:,0], data_class2[:,1], c='b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1. (3pts) Start by coding the sigmoid activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================================\n",
    "# Step I : Activation \n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    \n",
    "    '''As in the one hidden layer case, start by defining a function that given a dataset \n",
    "    X return the value sigma(x) = 1/(1+e^{-x}) as well as the gradient of this function'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return f, g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2. (6pts) Code the forward propagation of an input $X$ through a network with weights $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================================\n",
    "# Step II : Forward Propagation\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "def Forward_prop(X, w, network_size):\n",
    "    \n",
    "    '''Then we want to write the function that takes as input the dataset X \n",
    "    and the vector of targets t and returns the output to a network of size encoded \n",
    "    in network_size. Here network_size is a n_layer tuple in which each entry encode \n",
    "    the number of units in the layer. The output should be computed for the weight vector w'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return net_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3. (9pts) code the log-loss and its derivative through backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================================\n",
    "# Step III : Binary cross entropy and backpropagation\n",
    "# ================================================================================================\n",
    "\n",
    "    \n",
    "    \n",
    "def Logloss(X, t, w, network_size):\n",
    "    \n",
    "    '''This function should return the value of the log loss as well as the gradient of that loss\n",
    "    gradient computation should rely on backpropagation.'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Start by forward propagating the inputs through the network'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''While you forward propagate, '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return loss, loss_grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4. (4pts) Train your network using the minimize module from scipy. \n",
    "\n",
    "This modules requires you to specify the function Logloss wihch you coded above. The 'jac=True'parameter simply indicates that your function 'Logloss' should return both the loss that you want to minimize and the gradient of that loss. Minimize then apply gradient descent or a related optimization routine to this function.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================================\n",
    "# Step IV : Learning \n",
    "# ================================================================================================\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initialize your vector of weights with random gaussian weights \n",
    "\n",
    "\n",
    "# Then use the minimize module from scipy to train your network \n",
    "\n",
    "maxIter 500 # replace with your max number of iterations\n",
    "options = {'maxiter': maxIter}\n",
    "\n",
    "\n",
    "out = minimize(Logloss, w0, jac=True, method='TNC', options=options)\n",
    "\n",
    "Opti_weights = out.x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. (3pts) Once your network has been trained, use meshgrid to display the classification boundary below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = np.vstack((data_class1, data_class2))\n",
    "xmin = np.amin(data[:,0])\n",
    "xmax = np.amax(data[:,0])\n",
    "ymin = np.amin(data[:,1])\n",
    "ymax = np.amax(data[:,1])\n",
    "\n",
    "\n",
    "xequispaced = np.linspace(xmin, xmax, 100)\n",
    "yequispaced = np.linspace(ymin, ymax, 100)\n",
    "\n",
    "xx, yy = np.meshgrid(xequispaced, yequispaced)\n",
    "\n",
    "grid_data = np.vstack((xx.flatten(), yy.flatten())).T\n",
    "\n",
    "\n",
    "label_mesh = Forward_prop(X, Opti_weights, network_size) # use your forwardProp implementation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
